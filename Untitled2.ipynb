{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0203f0fd-1a01-499a-a059-208b67fb1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248d74c2-5e5d-40b5-ae93-d125c81d832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function if you need to\n",
    "def mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc117153-2156-42b1-86ab-4b826da9d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model (ensure the model is trained and saved correctly)\n",
    "model = tf.keras.models.load_model('lstm_temperature_humidity_model_with_time.h5', custom_objects={'mse': mse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36370ff8-f5d1-46ee-9800-5fb5a8ab713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dummy data for the last 30 time steps (this simulates the most recent data)\n",
    "num_temp_features = 7  # Number of temperature features\n",
    "num_hum_features = 7   # Number of humidity features\n",
    "num_time_features = 4  # hour_sin, hour_cos, month_sin, month_cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073bc112-16cc-48a4-82a8-058763d3304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy dataset\n",
    "dummy_temp = np.random.rand(30, num_temp_features)  # Random values for the last 30 time steps\n",
    "dummy_hum = np.random.rand(30, num_hum_features)    # Random values for the last 30 time steps\n",
    "dummy_time_features = np.zeros((30, num_time_features))  # Time features initialized to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d3443d-abfa-4aa7-8f80-0cda51f2bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current time\n",
    "now = pd.Timestamp.now(tz='UTC').floor('h')  # Get current time rounded down to the hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcb9810-17a6-4ab2-b450-bbc14bec1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the time features for the last 30 time steps\n",
    "for i in range(30):\n",
    "    current_time = now - pd.Timedelta(hours=i)\n",
    "    dummy_time_features[i, 0] = np.sin(2 * np.pi * current_time.hour / 24)  # hour_sin\n",
    "    dummy_time_features[i, 1] = np.cos(2 * np.pi * current_time.hour / 24)  # hour_cos\n",
    "    dummy_time_features[i, 2] = np.sin(2 * np.pi * current_time.month / 12)  # month_sin\n",
    "    dummy_time_features[i, 3] = np.cos(2 * np.pi * current_time.month / 12)  # month_cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e586d4f3-bb05-4fd8-9b61-19cd8d869947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "scaled_data = np.hstack((dummy_temp, dummy_hum, dummy_time_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e973259-5276-4913-bc47-a3592702897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create input for prediction\n",
    "def create_future_input(scaled_data):\n",
    "    last_sequence = scaled_data[-30:]  # Use the last 30 data points to create the input\n",
    "    future_inputs = []\n",
    "\n",
    "    # Generate future dates starting from today\n",
    "    today = pd.Timestamp.now(tz='UTC').floor('D')  # Current date rounded down to the day\n",
    "    future_dates = [today + pd.Timedelta(days=i) for i in range(7) for hour in [12, 23]]\n",
    "\n",
    "    for date in future_dates:\n",
    "        for hour in [12, 23]:  # Specify the hours we want to predict\n",
    "            # Create new features for the given future date and hour\n",
    "            hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "            hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "            month_sin = np.sin(2 * np.pi * date.month / 12)\n",
    "            month_cos = np.cos(2 * np.pi * date.month / 12)\n",
    "\n",
    "            # Create a new input sequence based on the last sequence\n",
    "            future_input = np.concatenate((\n",
    "                last_sequence.flatten(),\n",
    "                np.zeros((len(dummy_temp[0]) + len(dummy_hum[0]) + len(dummy_time_features[0]) - 4,))\n",
    "            ))\n",
    "\n",
    "            # Assign new values for time features\n",
    "            future_input[-4] = hour_sin\n",
    "            future_input[-3] = hour_cos\n",
    "            future_input[-2] = month_sin\n",
    "            future_input[-1] = month_cos\n",
    "\n",
    "            # Append the new input for the prediction\n",
    "            future_inputs.append(future_input)\n",
    "\n",
    "    # Reshape future inputs to be in the format (number of samples, 30 time steps, number of features)\n",
    "    return np.array(future_inputs).reshape(-1, 30, future_input.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9139aaf2-0349-45d3-91a8-e225b0eab012",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 15512 into shape (30,554)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create input for prediction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_future \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_future_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m, in \u001b[0;36mcreate_future_input\u001b[1;34m(scaled_data)\u001b[0m\n\u001b[0;32m     31\u001b[0m         future_inputs\u001b[38;5;241m.\u001b[39mappend(future_input)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Reshape future inputs to be in the format (number of samples, 30 time steps, number of features)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 15512 into shape (30,554)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create input for prediction\n",
    "X_future = create_future_input(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "596be1c6-5729-4823-9769-d495766fb16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step\n",
      "Predictions: [[0.34080192 0.34039816 0.35330498 0.37021407 0.4035673  0.39126772\n",
      "  0.28120503 0.74745846 0.7276199  0.7337414  0.718395   0.69755954\n",
      "  0.71112454 0.74440753]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the MSE function\n",
    "def mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "# Load the trained model with the custom loss function\n",
    "model = load_model('lstm_temperature_humidity_model_with_time.h5', custom_objects={'mse': mse})\n",
    "\n",
    "# Simulate last known temperature and humidity values\n",
    "last_known_temp = np.random.rand(7)  # Replace with actual last known temperature values\n",
    "last_known_hum = np.random.rand(7)    # Replace with actual last known humidity values\n",
    "\n",
    "# Function to prepare input data\n",
    "def prepare_input_data(current_time, last_known_values):\n",
    "    input_data = []\n",
    "    \n",
    "    # Ensure last_known_values has both temperature and humidity values\n",
    "    last_known_temp = last_known_values[:7]  # First 7 values are temperature\n",
    "    last_known_hum = last_known_values[7:]   # Last 7 values are humidity\n",
    "    \n",
    "    for i in range(7):  # For the next 7 days\n",
    "        for hour in [12, 23]:  # For 12:00 and 23:00\n",
    "            future_time = current_time + timedelta(days=i, hours=hour)\n",
    "            features = {\n",
    "                'hour_sin': np.sin(2 * np.pi * hour / 24),\n",
    "                'hour_cos': np.cos(2 * np.pi * hour / 24),\n",
    "                'month_sin': np.sin(2 * np.pi * future_time.month / 12),\n",
    "                'month_cos': np.cos(2 * np.pi * future_time.month / 12),\n",
    "            }\n",
    "            # Concatenate last known temp and humidity with the new features\n",
    "            input_data.append(np.concatenate([last_known_temp, \n",
    "                                               last_known_hum,\n",
    "                                               [features['hour_sin'], \n",
    "                                                features['hour_cos'], \n",
    "                                                features['month_sin'], \n",
    "                                                features['month_cos']]]))\n",
    "\n",
    "    # Convert to array and reshape for the model\n",
    "    input_array = np.array(input_data).reshape((1, 14, 18))  # Reshaping to (1, 14, 18)\n",
    "    return input_array\n",
    "# Simulate last known temperature and humidity values (replace with actual last known values)\n",
    "last_known_temp = np.random.rand(7)  # Random temperature values for testing\n",
    "last_known_hum = np.random.rand(7)    # Random humidity values for testing\n",
    "last_known_values = np.concatenate([last_known_temp, last_known_hum])\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "# Prepare input data\n",
    "input_data = prepare_input_data(current_time, last_known_values)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(input_data)\n",
    "\n",
    "# Output predictions\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4c95a85-bcd4-4b2a-8f22-657529b6ed03",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm_temperature_humidity_model_with_time.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Function to create input data for prediction\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_prediction_input\u001b[39m(current_data, lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    190\u001b[0m         filepath,\n\u001b[0;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:155\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    151\u001b[0m training_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(training_config)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m )\n\u001b[0;32m    159\u001b[0m saving_utils\u001b[38;5;241m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:143\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[1;34m(training_config, custom_objects)\u001b[0m\n\u001b[0;32m    141\u001b[0m loss_config \u001b[38;5;241m=\u001b[39m training_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43m_deserialize_nested_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# Ensure backwards compatibility for losses in legacy H5 files\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m _resolve_compile_arguments_compat(loss, loss_config, losses)\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:202\u001b[0m, in \u001b[0;36m_deserialize_nested_config\u001b[1;34m(deserialize_fn, config)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_single_object(config):\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    205\u001b[0m         k: _deserialize_nested_config(deserialize_fn, v)\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    207\u001b[0m     }\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\losses\\__init__.py:149\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.deserialize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeserialize\u001b[39m(name, custom_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserializes a serialized loss class/function instance.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        A Keras `Loss` instance or a loss function.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_OBJECTS_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:575\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_objects[config], types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m--> 575\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[43mserialize_with_public_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_module_name\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m    582\u001b[0m             serialize_with_public_class(\n\u001b[0;32m    583\u001b[0m                 module_objects[config], inner_config\u001b[38;5;241m=\u001b[39minner_config\n\u001b[0;32m    584\u001b[0m             ),\n\u001b[0;32m    585\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    586\u001b[0m         )\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:678\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    677\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m inner_config\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "File \u001b[1;32m~\\Documents\\venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:812\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[1;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 812\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('lstm_temperature_humidity_model_with_time.h5')\n",
    "\n",
    "# Function to create input data for prediction\n",
    "def create_prediction_input(current_data, lookback=30):\n",
    "    return np.array([current_data[-lookback:]])\n",
    "\n",
    "# Function to generate the next 7 days' datetime features at 12:00 and 23:00\n",
    "def generate_future_datetime_features(start_date, days=7):\n",
    "    future_dates = []\n",
    "    for i in range(1, days + 1):\n",
    "        future_dates.append(start_date + timedelta(days=i, hours=12))\n",
    "        future_dates.append(start_date + timedelta(days=i, hours=23))\n",
    "    return future_dates\n",
    "\n",
    "# Function to generate cyclical time features\n",
    "def generate_time_features(dates):\n",
    "    features = []\n",
    "    for date in dates:\n",
    "        hour_sin = np.sin(2 * np.pi * date.hour / 24)\n",
    "        hour_cos = np.cos(2 * np.pi * date.hour / 24)\n",
    "        month_sin = np.sin(2 * np.pi * date.month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * date.month / 12)\n",
    "        features.append([hour_sin, hour_cos, month_sin, month_cos])\n",
    "    return np.array(features)\n",
    "\n",
    "# Synthetic current data (scaled values, assuming the previous data processing)\n",
    "# This is a placeholder; you should replace this with the actual scaled data you have\n",
    "lookback = 30\n",
    "current_temp_hum = np.random.rand(lookback, 14)  # 7 temperature + 7 humidity features\n",
    "current_time_features = np.random.rand(lookback, 4)  # 4 time features (hour_sin, hour_cos, month_sin, month_cos)\n",
    "current_data = np.hstack((current_temp_hum, current_time_features))\n",
    "\n",
    "# Get the start date for prediction (current date and time)\n",
    "start_date = datetime.now()\n",
    "\n",
    "# Generate future datetime features\n",
    "future_dates = generate_future_datetime_features(start_date)\n",
    "future_time_features = generate_time_features(future_dates)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for time_feature in future_time_features:\n",
    "    # Combine current data with new time features\n",
    "    current_data = np.hstack((current_data[:, :-4], np.tile(time_feature, (current_data.shape[0], 1))))\n",
    "    X_pred = create_prediction_input(current_data)\n",
    "    pred = model.predict(X_pred)\n",
    "    predictions.append(pred[0])\n",
    "    # Update current data with the new prediction\n",
    "    current_data = np.vstack((current_data, np.hstack((pred, time_feature.reshape(1, -1)))))[1:]\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Reverse scaling (assuming MinMaxScaler was used and we have the original min and max values)\n",
    "# Placeholder scalers; replace these with the actual fitted scalers\n",
    "scaler_temp = MinMaxScaler()\n",
    "scaler_hum = MinMaxScaler()\n",
    "\n",
    "# Assuming the min and max values used for scaling were (0, 1) for simplicity\n",
    "scaler_temp.fit(np.random.rand(100, 7))  # Replace with actual temp data\n",
    "scaler_hum.fit(np.random.rand(100, 7))   # Replace with actual hum data\n",
    "\n",
    "# Reverse scaling to get actual temperature and humidity values\n",
    "predicted_temp = scaler_temp.inverse_transform(predictions[:, :7])\n",
    "predicted_hum = scaler_hum.inverse_transform(predictions[:, 7:14])\n",
    "\n",
    "# Prepare the final results\n",
    "results = []\n",
    "for i, date in enumerate(future_dates):\n",
    "    results.append({\n",
    "        'datetime': date,\n",
    "        'predicted_temperature': predicted_temp[i],\n",
    "        'predicted_humidity': predicted_hum[i]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d83b9f17-bf1f-43f5-a65f-56bccb7e466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "                     datetime  \\\n",
      "0  2024-10-10 17:30:47.658428   \n",
      "1  2024-10-11 04:30:47.658428   \n",
      "2  2024-10-11 17:30:47.658428   \n",
      "3  2024-10-12 04:30:47.658428   \n",
      "4  2024-10-12 17:30:47.658428   \n",
      "5  2024-10-13 04:30:47.658428   \n",
      "6  2024-10-13 17:30:47.658428   \n",
      "7  2024-10-14 04:30:47.658428   \n",
      "8  2024-10-14 17:30:47.658428   \n",
      "9  2024-10-15 04:30:47.658428   \n",
      "10 2024-10-15 17:30:47.658428   \n",
      "11 2024-10-16 04:30:47.658428   \n",
      "12 2024-10-16 17:30:47.658428   \n",
      "13 2024-10-17 04:30:47.658428   \n",
      "\n",
      "                                predicted_temperature  \\\n",
      "0   [0.48897615, 0.48267424, 0.49190372, 0.5075157...   \n",
      "1   [0.38993418, 0.3673504, 0.37453157, 0.39143878...   \n",
      "2   [0.47046664, 0.46387172, 0.47317082, 0.4905741...   \n",
      "3   [0.35446954, 0.331526, 0.33823475, 0.36476642,...   \n",
      "4   [0.4559825, 0.44887796, 0.4578567, 0.47814474,...   \n",
      "5   [0.33919573, 0.31930295, 0.3247046, 0.3554937,...   \n",
      "6   [0.44311154, 0.43615457, 0.4448018, 0.46793833...   \n",
      "7   [0.33747396, 0.31988987, 0.3243848, 0.35596222...   \n",
      "8   [0.4339957, 0.4263609, 0.43447465, 0.46075705,...   \n",
      "9   [0.34253636, 0.32665336, 0.3304082, 0.3621454,...   \n",
      "10  [0.42689526, 0.41828954, 0.4260722, 0.4557449,...   \n",
      "11  [0.3443843, 0.3304221, 0.3333913, 0.36662692, ...   \n",
      "12  [0.41809925, 0.40855184, 0.4164079, 0.44997868...   \n",
      "13  [0.3374915, 0.3244186, 0.32581374, 0.36186558,...   \n",
      "\n",
      "                                   predicted_humidity  \n",
      "0   [0.79186654, 0.7727241, 0.7556625, 0.7689243, ...  \n",
      "1   [0.60921204, 0.5871883, 0.5749978, 0.5868225, ...  \n",
      "2   [0.8288138, 0.81164306, 0.7959248, 0.8074171, ...  \n",
      "3   [0.66106474, 0.6376885, 0.62607247, 0.63440263...  \n",
      "4   [0.85187906, 0.8364302, 0.8207447, 0.83217573,...  \n",
      "5   [0.6981406, 0.67431396, 0.6611832, 0.6707301, ...  \n",
      "6   [0.8693729, 0.85573095, 0.8391816, 0.8515253, ...  \n",
      "7   [0.7258672, 0.7026877, 0.68778926, 0.69937474,...  \n",
      "8   [0.88440007, 0.87277037, 0.8557936, 0.8684726,...  \n",
      "9   [0.75127894, 0.7297553, 0.7127292, 0.726362, 0...  \n",
      "10  [0.89870304, 0.88951755, 0.87174195, 0.8845411...  \n",
      "11  [0.7797986, 0.76104164, 0.74219036, 0.7572233,...  \n",
      "12  [0.9166039, 0.9106696, 0.89105564, 0.9043637, ...  \n",
      "13  [0.8065708, 0.78936106, 0.7704371, 0.78606004,...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Custom objects dictionary\n",
    "custom_objects = {'mse': tf.keras.losses.MeanSquaredError()}\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('lstm_temperature_humidity_model_with_time.h5', custom_objects=custom_objects)\n",
    "\n",
    "# Function to create input data for prediction\n",
    "def create_prediction_input(current_data, lookback=30):\n",
    "    return np.array([current_data[-lookback:]])\n",
    "\n",
    "# Function to generate the next 7 days' datetime features at 12:00 and 23:00\n",
    "def generate_future_datetime_features(start_date, days=7):\n",
    "    future_dates = []\n",
    "    for i in range(1, days + 1):\n",
    "        future_dates.append(start_date + timedelta(days=i, hours=12))\n",
    "        future_dates.append(start_date + timedelta(days=i, hours=23))\n",
    "    return future_dates\n",
    "\n",
    "# Function to generate cyclical time features\n",
    "def generate_time_features(dates):\n",
    "    features = []\n",
    "    for date in dates:\n",
    "        hour_sin = np.sin(2 * np.pi * date.hour / 24)\n",
    "        hour_cos = np.cos(2 * np.pi * date.hour / 24)\n",
    "        month_sin = np.sin(2 * np.pi * date.month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * date.month / 12)\n",
    "        features.append([hour_sin, hour_cos, month_sin, month_cos])\n",
    "    return np.array(features)\n",
    "\n",
    "# Synthetic current data (scaled values, assuming the previous data processing)\n",
    "# This is a placeholder; you should replace this with the actual scaled data you have\n",
    "lookback = 30\n",
    "current_temp_hum = np.random.rand(lookback, 14)  # 7 temperature + 7 humidity features\n",
    "current_time_features = np.random.rand(lookback, 4)  # 4 time features (hour_sin, hour_cos, month_sin, month_cos)\n",
    "current_data = np.hstack((current_temp_hum, current_time_features))\n",
    "\n",
    "# Get the start date for prediction (current date and time)\n",
    "start_date = datetime.now()\n",
    "\n",
    "# Generate future datetime features\n",
    "future_dates = generate_future_datetime_features(start_date)\n",
    "future_time_features = generate_time_features(future_dates)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for time_feature in future_time_features:\n",
    "    # Combine current data with new time features\n",
    "    current_data = np.hstack((current_data[:, :-4], np.tile(time_feature, (current_data.shape[0], 1))))\n",
    "    X_pred = create_prediction_input(current_data)\n",
    "    pred = model.predict(X_pred)\n",
    "    predictions.append(pred[0])\n",
    "    # Update current data with the new prediction\n",
    "    current_data = np.vstack((current_data, np.hstack((pred, time_feature.reshape(1, -1)))))[1:]\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Reverse scaling (assuming MinMaxScaler was used and we have the original min and max values)\n",
    "# Placeholder scalers; replace these with the actual fitted scalers\n",
    "scaler_temp = MinMaxScaler()\n",
    "scaler_hum = MinMaxScaler()\n",
    "\n",
    "# Assuming the min and max values used for scaling were (0, 1) for simplicity\n",
    "scaler_temp.fit(np.random.rand(100, 7))  # Replace with actual temp data\n",
    "scaler_hum.fit(np.random.rand(100, 7))   # Replace with actual hum data\n",
    "\n",
    "# Reverse scaling to get actual temperature and humidity values\n",
    "predicted_temp = scaler_temp.inverse_transform(predictions[:, :7])\n",
    "predicted_hum = scaler_hum.inverse_transform(predictions[:, 7:14])\n",
    "\n",
    "# Prepare the final results\n",
    "results = []\n",
    "for i, date in enumerate(future_dates):\n",
    "    results.append({\n",
    "        'datetime': date,\n",
    "        'predicted_temperature': predicted_temp[i],\n",
    "        'predicted_humidity': predicted_hum[i]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f410c50d-fc4b-498c-b797-668ae1a2cdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "                     datetime  \\\n",
      "0  2024-10-10 17:32:28.056506   \n",
      "1  2024-10-11 04:32:28.056506   \n",
      "2  2024-10-11 17:32:28.056506   \n",
      "3  2024-10-12 04:32:28.056506   \n",
      "4  2024-10-12 17:32:28.056506   \n",
      "5  2024-10-13 04:32:28.056506   \n",
      "6  2024-10-13 17:32:28.056506   \n",
      "7  2024-10-14 04:32:28.056506   \n",
      "8  2024-10-14 17:32:28.056506   \n",
      "9  2024-10-15 04:32:28.056506   \n",
      "10 2024-10-15 17:32:28.056506   \n",
      "11 2024-10-16 04:32:28.056506   \n",
      "12 2024-10-16 17:32:28.056506   \n",
      "13 2024-10-17 04:32:28.056506   \n",
      "\n",
      "                                predicted_temperature  \\\n",
      "0   [0.50782734, 0.51289153, 0.5267452, 0.5289846,...   \n",
      "1   [0.4353406, 0.43349883, 0.43449798, 0.43416202...   \n",
      "2   [0.48874, 0.4940497, 0.5067947, 0.5118252, 0.5...   \n",
      "3   [0.40022752, 0.39468884, 0.39654407, 0.4019701...   \n",
      "4   [0.4729088, 0.47721857, 0.4890302, 0.49636847,...   \n",
      "5   [0.37016425, 0.36178234, 0.36402655, 0.377442,...   \n",
      "6   [0.45822692, 0.4622717, 0.4731893, 0.4832497, ...   \n",
      "7   [0.34401822, 0.33755964, 0.33872205, 0.3603040...   \n",
      "8   [0.44432378, 0.4487975, 0.45886502, 0.4721917,...   \n",
      "9   [0.33419392, 0.32903507, 0.32914838, 0.3551891...   \n",
      "10  [0.43611392, 0.44033417, 0.4496216, 0.46593186...   \n",
      "11  [0.33945435, 0.33552814, 0.3349267, 0.36094156...   \n",
      "12  [0.42919546, 0.43275657, 0.44138935, 0.4608096...   \n",
      "13  [0.3432213, 0.3403979, 0.34011766, 0.366069, 0...   \n",
      "\n",
      "                                   predicted_humidity  \n",
      "0   [0.73598886, 0.7351683, 0.7303435, 0.727216, 0...  \n",
      "1   [0.5444194, 0.5384768, 0.54345196, 0.5480273, ...  \n",
      "2   [0.7843796, 0.7842664, 0.782815, 0.7750647, 0....  \n",
      "3   [0.59438133, 0.5902647, 0.5942154, 0.59531134,...  \n",
      "4   [0.8123182, 0.8131767, 0.8135429, 0.803777, 0....  \n",
      "5   [0.64647603, 0.6420542, 0.64473134, 0.6415801,...  \n",
      "6   [0.83378017, 0.83582485, 0.83687806, 0.8261765...  \n",
      "7   [0.68323576, 0.67640424, 0.6796507, 0.67346084...  \n",
      "8   [0.8530423, 0.8568943, 0.85744786, 0.846588, 0...  \n",
      "9   [0.70872617, 0.7006556, 0.70458263, 0.69663227...  \n",
      "10  [0.86718917, 0.87273556, 0.873082, 0.86183596,...  \n",
      "11  [0.72562575, 0.7186414, 0.72144645, 0.7142178,...  \n",
      "12  [0.8791592, 0.88676554, 0.8865429, 0.8749083, ...  \n",
      "13  [0.74507415, 0.74011844, 0.74037045, 0.7340662...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Custom objects dictionary\n",
    "custom_objects = {'mse': tf.keras.losses.MeanSquaredError()}\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('lstm_temperature_humidity_model_with_time.h5', custom_objects=custom_objects)\n",
    "\n",
    "# Function to create input data for prediction\n",
    "def create_prediction_input(current_data, lookback=30):\n",
    "    return np.array([current_data[-lookback:]])\n",
    "\n",
    "# Function to generate the next 7 days' datetime features at 12:00 and 23:00\n",
    "def generate_future_datetime_features(start_date, days=7):\n",
    "    future_dates = []\n",
    "    for i in range(1, days + 1):\n",
    "        future_dates.append(start_date + timedelta(days=i, hours=12))\n",
    "        future_dates.append(start_date + timedelta(days=i, hours=23))\n",
    "    return future_dates\n",
    "\n",
    "# Function to generate cyclical time features\n",
    "def generate_time_features(dates):\n",
    "    features = []\n",
    "    for date in dates:\n",
    "        hour_sin = np.sin(2 * np.pi * date.hour / 24)\n",
    "        hour_cos = np.cos(2 * np.pi * date.hour / 24)\n",
    "        month_sin = np.sin(2 * np.pi * date.month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * date.month / 12)\n",
    "        features.append([hour_sin, hour_cos, month_sin, month_cos])\n",
    "    return np.array(features)\n",
    "\n",
    "# Synthetic current data (scaled values, assuming the previous data processing)\n",
    "# This is a placeholder; you should replace this with the actual scaled data you have\n",
    "lookback = 30\n",
    "current_temp_hum = np.random.rand(lookback, 14)  # 7 temperature + 7 humidity features\n",
    "current_time_features = np.random.rand(lookback, 4)  # 4 time features (hour_sin, hour_cos, month_sin, month_cos)\n",
    "current_data = np.hstack((current_temp_hum, current_time_features))\n",
    "\n",
    "# Get the start date for prediction (current date and time)\n",
    "start_date = datetime.now()\n",
    "\n",
    "# Generate future datetime features\n",
    "future_dates = generate_future_datetime_features(start_date)\n",
    "future_time_features = generate_time_features(future_dates)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for time_feature in future_time_features:\n",
    "    # Combine current data with new time features\n",
    "    current_data = np.hstack((current_data[:, :-4], np.tile(time_feature, (current_data.shape[0], 1))))\n",
    "    X_pred = create_prediction_input(current_data)\n",
    "    pred = model.predict(X_pred)\n",
    "    predictions.append(pred[0])\n",
    "    # Update current data with the new prediction\n",
    "    current_data = np.vstack((current_data, np.hstack((pred, time_feature.reshape(1, -1)))))[1:]\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Reverse scaling (assuming MinMaxScaler was used and we have the original min and max values)\n",
    "# Placeholder scalers; replace these with the actual fitted scalers\n",
    "scaler_temp = MinMaxScaler()\n",
    "scaler_hum = MinMaxScaler()\n",
    "\n",
    "# Assuming the min and max values used for scaling were (0, 1) for simplicity\n",
    "scaler_temp.fit(np.random.rand(100, 7))  # Replace with actual temp data\n",
    "scaler_hum.fit(np.random.rand(100, 7))   # Replace with actual hum data\n",
    "\n",
    "# Reverse scaling to get actual temperature and humidity values\n",
    "predicted_temp = scaler_temp.inverse_transform(predictions[:, :7])\n",
    "predicted_hum = scaler_hum.inverse_transform(predictions[:, 7:14])\n",
    "\n",
    "# Prepare the final results\n",
    "results = []\n",
    "for i, date in enumerate(future_dates):\n",
    "    results.append({\n",
    "        'datetime': date,\n",
    "        'predicted_temperature': predicted_temp[i],\n",
    "        'predicted_humidity': predicted_hum[i]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Adjust the display of the DataFrame to show readable results\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c95f68-6d86-4e81-a2c8-27d92b6da2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
